# -*- coding: utf-8 -*-
"""FakeNewsDetectionUsingLSTMShort.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WjmaU5n1rfYAmrQWvv3cRby_jShrGVem

Importing primary dependencies for data fetch and cleaning
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import re

from pandas import read_csv
from gensim.parsing.preprocessing import remove_stopwords
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Dense,Embedding,LSTM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

tf.keras.utils.set_random_seed(135656)
pd.options.mode.chained_assignment=None

"""Various constants"""

max_features=512
max_len=2048
test_prop=0.25
epoch_count=4
batch_size=16
min_string_len=240

#'Fake News' Community Prediction Competition on Kaggle
#https://www.kaggle.com/c/fake-news/data

data2=read_csv(
    'https://raw.githubusercontent.com/MobiusYeeitus/FakeNewsDetection/main/datasets/set2/train.csv')

data2.sample(4)

"""Slicing Dataset 2"""

data2_s=data2.iloc[1000:2000]
data2_s

"""Declaration *clean_csv* function"""

def clean_string(string): #function to clean a string | to be used in clean_csv
    string=string.lower() #convert string to lowercase
    string=re.sub('\n|\r|\t','',string) #cleaning of whitespaces
    string=re.sub(r'[^\w\s]+','',string) #cleaning of symbols
    return string

def clean_csv_without_identity(df): #cleaning the CSV file
    print('Cleaning CSV.')
    df=df.dropna() #drop all tuples with empty attributes
    length=[]
    [length.append(len(str(text))) for text in df["text"]] #calculate length of an article
    df['length']=length #saves the length of the article in a new column
    df=df.drop(df['text'][df['length']<min_string_len].index,axis=0) #drop all tuples with less than min_string_len letters
    #df['text']=df['title']+" "+df['text'] #append text to title
    vfunc=np.vectorize(clean_string) #vectorize the clean_string function for use in pandas df
    df['text']=vfunc(df['text']) #apply the vectorized function to further clean the data
    df.reset_index() #reset all indexes of the dataset
    for x in range (len(df)):
        df['text'].iloc[x]=remove_stopwords(df['text'].iloc[x]) #cleaning stopwords
    df.rename(columns={'label':'identity'},inplace=True)
    print('Cleaning completed.')
    return df

def optimized_cleaning_without_identity(df): #optimized for less than 16GiB RAM
    print('Optimizing cleaning.')
    df_temp1=df.iloc[:int(len(df)/2),:]
    df_temp2=df.iloc[int(len(df)/2):,:]
    df_temp1_c=clean_csv_without_identity(df_temp1)
    df_temp2_c=clean_csv_without_identity(df_temp2)
    df_temp=pd.concat([df_temp1_c,df_temp2_c],ignore_index=True)
    return df_temp

"""Splitting up the datasets to conserve RAM, and avoid crashes during cleaning by using the optimized functions"""

data2_c=optimized_cleaning_without_identity(data2_s)

"""Before cleaning,"""

data2.tail(2)

"""After cleaning,"""

data2_c.tail(2)

"""Dropping all unnecessary columns"""

data2_c=data2_c[['text','identity']]

data_c=pd.concat([data2_c],ignore_index=True)

data_c=data_c[['text','identity']]
data_c.reset_index()
data_c.tail(4)

"""Visual representation"""

fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,6))
data_c['identity'].value_counts().plot.pie(explode=[0,0.1],colors=sns.color_palette("RdYlBu"),autopct='%1.1f%%',ax=ax1)
ax1.set_title('Percentage of identity')
sns.countplot(x='identity',data=data_c,ax=ax2,palette='RdYlBu')
ax2.set_title('Distribution of identity')
plt.show()

"""*Tokenizing* the dataset for use in the ANN (One_Hot Representation)"""

print('Tokenizing DF.')
tokenizer=Tokenizer(num_words=max_features,filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',split =' ') #filters is redundant, but still keeping if I missed any symbols previously
tokenizer.fit_on_texts(texts=data_c['text']) #fit
X=tokenizer.texts_to_sequences(texts=data_c['text'])
X=pad_sequences(sequences=X,maxlen=max_len,padding='pre') #padding the set of words
Y=data_c['identity'].values #identities (0/1)
V=tokenizer.word_index #vocabulary
print('Tokenized DF.')

"""Creating the model and compiling it"""

def create_lstm_model(max_features): #creating the internal structure of the simple LSTM model
        print('Creating LSTM model.')
        lstm_model=Sequential(name='Sierra')
        lstm_model.add(layer=Embedding(input_dim=max_features,output_dim=128,name='Echo'))
        lstm_model.add(layer=LSTM(units=128,name='Lima'))
        lstm_model.add(layer=Dense(1,activation='sigmoid',name ='Omega'))
        print('Model creation completed.')
        lstm_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
        print('Model compiled.')
        return lstm_model

lstm_model=create_lstm_model(max_features)

"""Summary of the model"""

lstm_model.summary()

"""Splitting the tokenized DF based on *test_prop*"""

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=test_prop)

"""Training the LSTM model"""

history=lstm_model.fit(X_train,Y_train,batch_size=batch_size,epochs=epoch_count,verbose=1,validation_split=test_prop,shuffle=1,use_multiprocessing=True)

"""Visualization of accuracy and loss using graph"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""Validation of the model"""

Y_pred=(lstm_model.predict(X_test,batch_size=batch_size,verbose=1,use_multiprocessing=True)>=0.5).astype(int)

accuracy_score(Y_test,Y_pred)

print(classification_report(Y_test,Y_pred))

lstm_model.save('models/ds2-short')

"""Using *ConfusionMatrixDisplay* function to visualize the confusion matrix"""

cm_y_pred=confusion_matrix(Y_test,Y_pred)
disp=ConfusionMatrixDisplay(cm_y_pred)
disp.plot()

