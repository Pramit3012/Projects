# -*- coding: utf-8 -*-
"""FakeNewsDetectionUsingLSTMStable.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iZkAsbTO2FDJ39uOLFO05s-2KaaIYsfj

create separate models for title prediction and/or text prediction and divide the dataset into title datasets and text datasets. for that need to redo

Importing primary dependencies for data fetch and cleaning
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

from pandas import read_csv

"""Various constants"""

max_features=10240
max_len=2048
test_prop=0.3
epoch_count=100
batch_size=512
min_string_len=240

"""Sources of the datasets are mentioned here"""

#'Fake and real news dataset' by Clement Bisaillon
#https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset

data1_fake=read_csv('https://raw.githubusercontent.com/MobiusYeeitus/FakeNewsDetection/main/datasets/set1/fake.csv')
data1_real=read_csv('https://raw.githubusercontent.com/MobiusYeeitus/FakeNewsDetection/main/datasets/set1/true.csv')

#'Fake News' Community Prediction Competition on Kaggle
#https://www.kaggle.com/c/fake-news/data

data2=read_csv(
    'https://raw.githubusercontent.com/MobiusYeeitus/FakeNewsDetection/main/datasets/set2/train.csv')

#'Getting Real about Fake News' by Meg Risdal
#https://www.kaggle.com/datasets/mrisdal/fake-news

data3=read_csv('https://raw.githubusercontent.com/MobiusYeeitus/FakeNewsDetection/main/datasets/set3/fake.csv')

#'Fake News' by Hassan Amin
#https://www.kaggle.com/datasets/hassanamin/textdb3

data4=read_csv('https://raw.githubusercontent.com/MobiusYeeitus/FakeNewsDetection/main/datasets/set4/fake_or_real_news.csv')

data1_fake.sample(4)

data1_real.sample(4)

data2.sample(4)

data3.sample(4)

data4.sample(4)

"""Compressing every cleanup job into the *clean_csv* function"""

import nltk

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud
stem=PorterStemmer()

from gensim.parsing.preprocessing import remove_stopwords
pd.options.mode.chained_assignment=None

"""Stemming not used after testing indicates reduced accuracy, suspecting nouns as cause"""

def stemming(string): #not using stemming, reduces accuracy with nouns
    review = re.sub('[^a-zA-Z]',' ',string)
    review = review.lower()
    review = review.split()
    review = [stem.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    return review

"""*clean_string* function for use in *clean_csv* function"""

def clean_string(string): #function to clean a string | to be used in clean_csv
    string=string.lower() #convert string to lowercase
    string=re.sub('\n|\r|\t','',string) #cleaning of whitespaces
    string=re.sub(r'[^\w\s]+','',string) #cleaning of symbols
    return string

"""Variations of *clean_csv* function for use in different kinds of datasets"""

def clean_csv_with_identity(df,identity): #cleaning the CSV file
    print('Cleaning CSV.')
    df=df.dropna() #drop all tuples with empty attributes
    length=[]
    [length.append(len(str(text))) for text in df["text"]] #calculate length of an article
    df['length']=length #saves the length of the article in a new column
    df=df.drop(df['text'][df['length']<min_string_len].index,axis=0) #drop all tuples with less than min_string_len letters
    #df['text']=df['title']+" "+df['text'] #append text to title
    vfunc=np.vectorize(clean_string) #vectorize the clean_string function for use in pandas df
    df['text']=vfunc(df['text']) #apply the vectorized function to further clean the data
    df['identity']=identity #add the identity (T/F) or (0/1) in a new attribute to the data
    df.reset_index() #reset all indexes of the dataset
    for x in range (len(df)):
        df['text'].iloc[x]=remove_stopwords(df['text'].iloc[x]) #cleaning stopwords
    print('Cleaning completed.')
    return df

def optimized_cleaning_with_identity(df,identity): #optimized for less than 16GiB RAM
    print('Optimizing cleaning.')
    df_temp1=df.iloc[:int(len(df)/2),:]
    df_temp2=df.iloc[int(len(df)/2):,:]
    df_temp1_c=clean_csv_with_identity(df_temp1,identity)
    df_temp2_c=clean_csv_with_identity(df_temp2,identity)
    df_temp=pd.concat([df_temp1_c,df_temp2_c],ignore_index=True)
    return df_temp

def clean_csv_without_identity(df): #cleaning the CSV file
    print('Cleaning CSV.')
    df=df.dropna() #drop all tuples with empty attributes
    length=[]
    [length.append(len(str(text))) for text in df["text"]] #calculate length of an article
    df['length']=length #saves the length of the article in a new column
    df=df.drop(df['text'][df['length']<min_string_len].index,axis=0) #drop all tuples with less than min_string_len letters
    #df['text']=df['title']+" "+df['text'] #append text to title
    vfunc=np.vectorize(clean_string) #vectorize the clean_string function for use in pandas df
    df['text']=vfunc(df['text']) #apply the vectorized function to further clean the data
    df.reset_index() #reset all indexes of the dataset
    for x in range (len(df)):
        df['text'].iloc[x]=remove_stopwords(df['text'].iloc[x]) #cleaning stopwords
    df.rename(columns={'label':'identity'},inplace=True)
    print('Cleaning completed.')
    return df

def optimized_cleaning_without_identity(df): #optimized for less than 16GiB RAM
    print('Optimizing cleaning.')
    df_temp1=df.iloc[:int(len(df)/2),:]
    df_temp2=df.iloc[int(len(df)/2):,:]
    df_temp1_c=clean_csv_without_identity(df_temp1)
    df_temp2_c=clean_csv_without_identity(df_temp2)
    df_temp=pd.concat([df_temp1_c,df_temp2_c],ignore_index=True)
    return df_temp

def clean_csv_with_label_replace(df,label_real,label_fake): #cleaning the CSV file
    print('Cleaning CSV.')
    df=df.dropna() #drop all tuples with empty attributes
    df['label'].replace(to_replace=str('REAL'),value=str('1'),inplace=True) #replace the labels with corresponding values (0/1)
    df['label'].replace(to_replace=str('FAKE'),value=str('0'),inplace=True)
    length=[]
    [length.append(len(str(text))) for text in df["text"]] #calculate length of an article
    df['length']=length #saves the length of the article in a new column
    df=df.drop(df['text'][df['length']<min_string_len].index,axis=0) #drop all tuples with less than min_string_len letters
    #df['text']=df['title']+" "+df['text'] #append text to title
    vfunc=np.vectorize(clean_string) #vectorize the clean_string function for use in pandas df
    df['text']=vfunc(df['text']) #apply the vectorized function to further clean the data
    df.rename(columns={'label':'identity'},inplace=True) #renaming column
    df.reset_index() #reset all indexes of the dataset
    for x in range (len(df)):
        df['text'].iloc[x]=remove_stopwords(df['text'].iloc[x]) #cleaning stopwords
    print('Cleaning completed.')
    return df

def optimized_cleaning_with_label_replace(df,label_real,label_fake): #optimized for less than 16GiB RAM
    print('Optimizing cleaning.')
    df_temp1=df.iloc[:int(len(df)/2),:]
    df_temp2=df.iloc[int(len(df)/2):,:]
    df_temp1_c=clean_csv_with_label_replace(df_temp1,label_real,label_fake)
    df_temp2_c=clean_csv_with_label_replace(df_temp2,label_real,label_fake)
    df_temp=pd.concat([df_temp1_c,df_temp2_c],ignore_index=True)
    return df_temp

"""Splitting up the datasets to conserve RAM, and avoid crashes during cleaning by using the optimized functions"""

data1_fake_c=optimized_cleaning_with_identity(data1_fake,0)

"""Before cleaning,"""

data1_fake.tail(2)

"""After cleaning"""

data1_fake_c.tail(2)

"""Visual representation"""

wc=WordCloud(background_color="white",max_words=100,max_font_size=256,random_state=42,width=1920,height=1080,stopwords=None)
wc.generate(' '.join(data1_fake_c['text']))
plt.imshow(wc)
plt.axis('off')
plt.show()

data1_real_c=optimized_cleaning_with_identity(data1_real,1)

"""Before cleaning,"""

data1_real.tail(2)

"""After cleaning"""

data1_real_c.tail(2)

"""Visual representation"""

wc=WordCloud(background_color="white",max_words=100,max_font_size=256,random_state=42,width=1920,height=1080,stopwords=None)
wc.generate(' '.join(data1_real_c['text']))
plt.imshow(wc)
plt.axis('off')
plt.show()

data2_c=optimized_cleaning_without_identity(data2)

"""Before cleaning,"""

data2.tail(2)

"""After cleaning"""

data2_c.tail(2)

"""Visual representation"""

wc=WordCloud(background_color="white",max_words=100,max_font_size=256,random_state=42,width=1920,height=1080,stopwords=None)
wc.generate(' '.join(data2_c['text']))
plt.imshow(wc)
plt.axis('off')
plt.show()

data3_c=optimized_cleaning_with_identity(data3,0)

"""Before cleaning,"""

data3.tail(2)

"""After cleaning"""

data3_c.tail(2)

"""Visual representation"""

wc=WordCloud(background_color="white",max_words=100,max_font_size=256,random_state=42,width=1920,height=1080,stopwords=None)
wc.generate(' '.join(data3_c['text']))
plt.imshow(wc)
plt.axis('off')
plt.show()

data4_c=optimized_cleaning_with_label_replace(data4,'REAL','FAKE')

"""Before cleaning,"""

data4.tail(2)

"""After cleaning"""

data4_c.tail(2)

"""Visual representation"""

wc=WordCloud(background_color="white",max_words=100,max_font_size=256,random_state=42,width=1920,height=1080,stopwords=None)
wc.generate(' '.join(data4_c['text']))
plt.imshow(wc)
plt.axis('off')
plt.show()

"""Dropping all unnecessary columns"""

data1_fake_c=data1_fake_c[['text','identity']]
data1_real_c=data1_real_c[['text','identity']]
data2_c=data2_c[['text','identity']]
data3_c=data3_c[['text','identity']]
data4_c=data4_c[['text','identity']]

data1_c=pd.concat([data1_real_c,data1_fake_c],ignore_index=True)

data1_c.tail(2)

"""Finalizing dataset by combining all cleaned datasets"""

data_c=pd.concat([data1_c,data2_c,data3_c],ignore_index=True)

data_c=data_c[['text','identity']]
data_c.reset_index()
data_c.tail(4)

"""Visual representation"""

fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,6))
data_c['identity'].value_counts().plot.pie(explode=[0,0.1],colors=sns.color_palette("RdYlBu"),autopct='%1.1f%%',ax=ax1)
ax1.set_title('Percentage of identity')
sns.countplot(x='identity',data=data_c,ax=ax2,palette='RdYlBu')
ax2.set_title('Distribution of identity')
plt.show()

wc=WordCloud(background_color="white",max_words=100,max_font_size=256,random_state=42,width=1920,height=1080,stopwords=None)
wc.generate(' '.join(data_c['text']))
plt.imshow(wc)
plt.axis('off')
plt.show()

"""Importing dependencies for ANN and Representation"""

import tensorflow as tf
import itertools

from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Dense,Embedding,LSTM,Conv1D,MaxPool1D,Dropout,GRU
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix

tf.keras.utils.set_random_seed(135656)

"""*Tokenizing* the dataset for use in the ANN (One_Hot Representation)"""

print('Tokenizing DF.')
tokenizer=Tokenizer(num_words=max_features,filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',split =' ') #filters is redundant, but still keeping if I missed any symbols previously
tokenizer.fit_on_texts(texts=data_c['text']) #fit
X=tokenizer.texts_to_sequences(texts=data_c['text'])
X=pad_sequences(sequences=X,maxlen=max_len,padding='pre') #padding the set of words
Y=data_c['identity'].values #identities (0/1)
V=tokenizer.word_index #vocabulary
print('Tokenized DF.')

"""Creating the model and compiling it"""

def create_lstm_model(max_features): #creating the internal structure of the simple LSTM model
        print('Creating LSTM model.')
        lstm_model=Sequential(name='Sierra')
        lstm_model.add(layer=Embedding(input_dim=max_features,output_dim=128,name='Echo'))
        lstm_model.add(layer=LSTM(units=128,name='Lima'))
        lstm_model.add(layer=Dense(1,activation='sigmoid',name ='Omega'))
        print('Model creation completed.')
        lstm_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
        print('Model compiled.')
        return lstm_model

lstm_model=create_lstm_model(max_features)

"""Summary of the model"""

lstm_model.summary()

"""Splitting the tokenized DF based on *test_prop*"""

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=test_prop)

"""Training the LSTM model"""

history=lstm_model.fit(X_train,Y_train,batch_size=batch_size,epochs=epoch_count,verbose=1,validation_split=test_prop,shuffle=1,use_multiprocessing=True)

"""Visualization of accuracy and loss using graph"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

"""Validation of the model"""

Y_pred=(lstm_model.predict(X_test,batch_size=batch_size,verbose=1,use_multiprocessing=True)>=0.5).astype(int)

accuracy_score(Y_test,Y_pred)

print(classification_report(Y_test,Y_pred))

lstm_model.save('models/ds1-2-3-4_combined_epoch-100_extensive_v2')

saved_lstm_model=tf.keras.saving.load_model('models/ds1-2-3-4_combined_epoch-100_extensive_v2')

y_pred=(saved_lstm_model.predict(X_test,batch_size=batch_size,verbose=1,use_multiprocessing=True)>=0.5).astype(int)

accuracy_score(Y_test,y_pred)

"""Importing *ConfusionMatrixDisplay* function to visualize the confusion matrix"""

from sklearn.metrics import ConfusionMatrixDisplay

cm_y_pred=confusion_matrix(Y_test,y_pred)
disp=ConfusionMatrixDisplay(cm_y_pred)
disp.plot()

print(classification_report(Y_test,y_pred))

string_predict1=pd.DataFrame()
string_predict1=data_c.iloc[20000:24000]
string_predict1

X=tokenizer.texts_to_sequences(texts=string_predict1['text'])
X=pad_sequences(sequences=X,maxlen=max_len,padding='pre')
string_predict1_res=(saved_lstm_model.predict(X,batch_size=batch_size,verbose=1,use_multiprocessing=True)>=0.5).astype(int)

accuracy_score(string_predict1_res,string_predict1['identity'])

cm_string_predict1_res=confusion_matrix(string_predict1_res,string_predict1['identity'])
disp=ConfusionMatrixDisplay(cm_string_predict1_res)
disp.plot()

string_predict2=pd.DataFrame()
string_predict2=data_c.iloc[40000:48000]
string_predict2

X=tokenizer.texts_to_sequences(texts=string_predict2['text'])
X=pad_sequences(sequences=X,maxlen=max_len,padding='pre')
string_predict2_res=(saved_lstm_model.predict(X,batch_size=batch_size,verbose=1,use_multiprocessing=True)>=0.5).astype(int)

accuracy_score(string_predict2_res,string_predict2['identity'])

cm_string_predict2_res=confusion_matrix(string_predict2_res,string_predict2['identity'])
disp=ConfusionMatrixDisplay(cm_string_predict2_res)
disp.plot()

"""Work in Progress below this part"""

string_predict3=pd.DataFrame()
string_predict3['text']=''

